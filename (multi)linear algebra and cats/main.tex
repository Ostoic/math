\documentclass[12pt]{article}
\usepackage{amsmath, amsthm, amscd, epsfig, amssymb, blindtext}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pst-node}
\usepackage{tikz-cd} 
\usepackage{xifthen}
\usepackage{xparse}

\include{commands}

\topmargin  = .8cm
\textwidth  = 164mm
\textheight = 219mm
\oddsidemargin = 0mm
\evensidemargin = 0mm
\parskip = 0cm

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{definition*}{Definition}
\newtheorem{example}{Example}[section]
\newtheorem*{example*}{Example}
\newtheorem{remark}{Remark}[section]
\newtheorem*{remark*}{Remark}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{proposition}[theorem] {Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}

\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}

\renewcommand{\baselinestretch}{1.6}

\numberwithin{equation}{section}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exm}[thm]{Example}

\baselineskip 18pt
  
\title{Review of Linear Algebra}

\author{Shaun Ostoic}
\date{\today}

\begin{document}
\maketitle

\section{Free Objects in Concrete Categories}

When working with modules, a free module is defined as a module with a basis. A certain universal property may be considered involving free modules which gives an equivalent perspective of free modules. It is through this universal property that we generalize the notion of "free-ness". 

\begin{definition}
Let $\mathscr{C}$ be a concrete category (a category whose objects we assume are sets), and let $ A $ be a set. An object $ F $ together with a set map $ i : A \to F $ is \textit{free} on $ A $ if for any object $ Z $ and set map $ f : A \to Z $, there is a unique morphism $ \bar{f}: F \to Z $ such that $ \bar{f} i = f $ as sets. The set $ A $ is called a \textit{basis} for $ F $.
\end{definition}

\begin{remark}
Given a concrete category $ \mathscr{C} $, the functor $ F : \cat{Set} \to \cat{C}$ sending $ B $ to the free object in $ \cat{C} $ with basis $ B $ is called the \textit{free functor}.
\end{remark}

When it makes sense, it is natural to think that if we have bases in bijection, then the object generated by those bases should be isomorphic. This is true in familiar categories such as \cat{Ab}, $ R $-\cat{Mod}, etc. It is interesting that this holds in general for concrete categories.

\begin{theorem}
Let $ A, A' \in \cat{Set}$, and $ F,  F' \in \mathscr{C}$ be free on $ A$ and $ A' $, respectively. If $ \order{A} = \order{A'} $, then $ F \cong F' $ in $ \mathscr{C} $.
\end{theorem}

\begin{proof}
The free functor $ F : \cat{Set} \to \mathscr{C} $ preserves isomorphism, so if $ B \cong B' $ in $ \cat{Set} $ then $ F(B) \cong F(B') $ in $ \mathscr{C}$.
\end{proof}

An immediate application of this theorem in the familiar category of modules is given in the following corollary.

\begin{corollary*}
If $ F $ is any free $ R $-module with basis $ A $, then $ F \cong F^{\oplus A} $.
\end{corollary*}

In our quest to make extensive use of free objects in concrete categories, we would like to generalize some analagous properties of morphisms. One such property I've discovered is that of generating sets. Intuitively, a subset of some concrete object can be used to generate another concrete subobject by applying all possible operations on the elements in the generating set. This idea is used repeatedly for groups, rings, modules, vector spaces, and so on. But really this is just a form of viewing the free object inside another object.

\begin{definition} \label{def_concrete_gen}
Let $ Z \in \cat{C}$ be a concrete object, let $ S \subset Z$ be a set, and consider the free object $ F $ on $ S $. The object $ \cyclic{S} $ generated by $ S $ is the image of the unique morphism $ \varphi : F \to Z $ in the universal property of the free object. That is, $ \cyclic{S} := \varphi (F) $.
\end{definition}

\begin{example}
In the category of $ R $-modules, it is easy to see that $ \cyclic{S} = \{ \sum r_i s_i \mid r_i \in R, s_i \in S\} $ from this definition since the unique morphism from the free $ R $-module extends $ S $ linearly to the ambient module.
\end{example}

\begin{proposition} \label{concrete_gen}
Let $ \varphi, \psi : Z \to Z'  $ be morphisms in a concrete category and suppose $ Z = \cyclic{S} $, for some generating set $ S $. If $ \varphi  $ and $ \psi $ agree on $ S $, then $ \varphi = \psi $.
\end{proposition}

\begin{proof}
Let us set-up the diagram for the free object.

\[ \begin{tikzcd}[column sep=large]
	S \arrow[rrr, bend left = 20, "j"] \arrow[d, "\iota"] \arrow[drrr, near start, "\varphi \iota \text{ = } \psi \iota"] & & & F \arrow[dlll, dashed, near start, crossing over, "\exists !f"] \arrow[d, dashed, " \exists !g"]\\
	Z \arrow[rrr, shift left, bend right=25, "\varphi"] \arrow[rrr, shift right, bend right=25, "\psi"'] & & & Z' 
\end{tikzcd}\]
In the above diagram, we invoke the universal property twice. First is for the set maps $ j : S \to F $ and inclusion $ \iota : S \to Z $, from which we get the unique morphism $ f : F \to Z $, satisfying $ f j = \iota $. From the assumption it follows that $ \varphi \iota = \psi \iota $, so we invoke the universal property again for the set maps $ j : S \to F $ and $ \varphi \iota = \psi \iota : S \to Z' $, from which we have the unique morphism $ g : F \to Z' $, satisfying $ g j = \varphi \iota = \psi \iota $. If we can show that the morphism $ \varphi f : F \to Z' $ satisfies the same universal property as $ g $ does, then uniqueness would imply that $ \varphi f = g $; similarly we would have $ \psi f = g $. Indeed, $ (\varphi f) j = \varphi (f j) = \varphi \iota = \psi \iota = \psi (fj) = (\psi f) j$, so $ \varphi f = g = \psi f$. Moreover, $ Z = \cyclic{S} = im f $ which means $ f $ is surjective, hence epic. $ \varphi f = \psi f $ then yields $ \varphi = \psi $, as desired.
\end{proof}

\begin{remark}

An alternate way to view free objects is to enhance our focus on the morphisms between objects in a category. One way to do so is to work within another category that is related to $ \mathscr{C} $, where objects are morphisms, and where morphisms are diagrams. However, we want objects to have the property of being set-maps, while morphisms should be "true" morphisms, as they are in $ \mathscr{C} $.

We construct the category $ \mathscr{C}^A $ for a set $ A $ whose objects consist of set maps $ A \to Z$ denoted $ (f, Z) $. Morphisms are defined as $  (f_1, Z_1) \to (f_2, Z_2) $, which consists of a morphism $ \varphi : Z_1 \to Z_2 $, such that the following diagram commutes.
\[ \begin{tikzcd}
		A \arrow[dr, "f_2"'] \arrow[r, "f_1"] & Z_1 \arrow[d, "\varphi"]\\
											 & Z_2 
\end{tikzcd}\]
Composition is accomplished as follows. Let $ (j_1, Z_1) \xrightarrow{\varphi} (j_2, Z_2) \xrightarrow{\psi} (j_3, Z_3)$ be morphisms. Then we have the composed diagrams
\[ \begin{tikzcd}
	Z_1 \arrow[r, "\varphi"] & Z_2 \arrow[r, "\psi"] & Z_3\\
		& A \arrow[ul, "j_1"] \arrow[u, "j_2"] \arrow[ur, "j_3"'] &
\end{tikzcd}\]
where commutativity of the inner two triangles implies commutativity of the outer triangle. Indeed, we have $ \varphi j_1 = j_2 $ and $ \psi j_2 = j_3 $. Thus, $ \psi \varphi j_1 = \psi (\varphi j_1) = \psi j_2 = j_3 $. The identity morphism in this category comes from the underlying category. Given an object $ Z $ in $ F $, the identity morphism $ 1_Z : Z \to Z $ is the identity morphism in the coslice category. Certainly, $ (j, Z) \xrightarrow{1_Z} (j, Z)$ yields $ 1_Z j = j $. Now we can consider what isomorphism means in this category. We take $ (j_3, Z_3) = (j_1, Z_1) $. Assuming $ (j_1, Z_1) \cong (j_2, Z_2) $ and assuming $ \varphi, \psi $ are the morphisms that come with objects being isomorphic, we have the composition of the morphisms being $ \psi \varphi = 1_{Z_1} $ and $ \varphi \psi = 1_{Z_2} $, proving $ Z_1 \cong Z_2 $ in our original category. Thus, objects that are isomorphic in this new category $ \mathscr{C}^A $ produce morphisms in the original category that satisfy extra properties, namely, preserving the set maps throughout the diagram.

Considering $ (i, F^{\oplus A}) $ as an object of $ \mathscr{C}^A $, we have shown that for any object $ (f, Z) $ in $ F^A $ there is a unique morphism $ (i, F^{\oplus A}) \xrightarrow{\bar{f}} (f, Z) $. This shows that $ F^{\oplus A} $ is part of an initial object in $ \mathscr{C}^A $. It can be shown that any two initial objects in a category are isomorphic, from which we deduce that all $ R $-modules that are free on $ A $ are isomorphic. We have also shown that isomorphism in this category is stronger than it is in $ \mathscr{C} $, so isomorphism in $ \mathscr{C} $ follows.
\end{remark}

\section{Free Objects in $ R $-\cat{Mod}}
\begin{remark}
We consider rings and ring homomorphisms to be unital.
\end{remark}

\begin{definition}
A basis for an $ R $-module $ M $ is a set $ A \subset M $ such that every $ m \in M $ can be written uniquely as a linear combination of elements from $ A $.
\end{definition}

\begin{theorem}
For every set $ A $, there exists an $ R $-module with basis $ A $.
\end{theorem}

\begin{proof}
Here we give a long-winded and excruitatingly-detailed construction of a free module over a ring with $ 1 $.
Let $ R $ be a ring, and let $ A $ be a nonempty set. We define 
\[ F^{\oplus A} := \{ x: A \to R \mid \supp{x} \text{ is finite} \} \]
which will serve as our future free $ R $-module. Define addition and scalar multiplication on $ F^{\oplus A} $ pointwise:
\begin{align*}
	(x + y)(a) &= x(a) + y(a)\\
	(r x)(a)   &= r x(a)
\end{align*}
for $ x, y \in F^{\oplus A} $ and $ r \in R $, and let $ i : A \to F^{\oplus A} $ be the map whose image elements are the functions 
\[ \alpha_a(a') = \begin{cases}
		1, &\text{if } a'= a\\
		0, &\text{ otherwise}
\end{cases}\]
It is easy to see that $ i $ is injective. For $ a_1, a_2 \in A $, if $ \alpha_{a_1}(a') = \alpha_{a_2}(a') $ for any $ a' \in A $, it follows that $ \alpha_{a_1}(a_2) = \alpha_{a_2}(a_2) = 1 $; hence $ \alpha_{a_1}(a_2)  = 1 $ and $ a_1 = a_2 $.

We can think of $ i(a) $ as the ordered pair of all zeros, except for the $ a $'th column being a $ 1 $. For simplicity we denote $ i(a) $ by $ a $ from now on. From here we will show that $ i(A) $ generates $ F^{\oplus A} $. Let $ x \in F^{\oplus A} $, whose support consists of $ a_1, \ldots, a_n $; denote $ x(a_j) $ by $ x_j $. By finiteness of the support, we can form the sum
\begin{align*}
	y := \sum_{j = 1}^n x_j a_j
\end{align*}
For any $ a \in A $, we see that if $ a \notin \supp{y} $, then
\begin{align*}
	y(a) &= (x_1 a_1)(a) + \ldots + (x_n a_n)(a)\\
		 &= x_1 (a_1(a)) + \ldots + x_n (a_n(a))\\
		 &= x_1 \cdot 0 + \ldots + x_n \cdot 0\\
		 &= 0.
\end{align*} 
However, without loss of generality, if $ a = a_1 $, then
\begin{align*}
	y(a_1) &= x_1 a_1(a_1) + x_2 a_2(a_1) + \ldots + x_n a_n(a_1)\\
		   &= x_1 \cdot 1 + x_2 \cdot 0 + \ldots + x_n \cdot 0\\
		   &= x_1.
\end{align*}
Thus, $ y(a_j) = x_j = x(a_j) $ for any $ a_j \in \supp{x}$. Clearly then, $ y(a) = x(a)$ for all $ a \in A $ so that $ y = x $ and hence $ i(A) $ generates $ F^{\oplus A} $. Next we show that $ i(A) $ is a linearly independent set. Suppose $ x = \sum x_i a_i = 0 $. Then $ x(a_i) = x_i \cdot a_i(a_i) = x_i \cdot 1  = x_i = 0$. Since $ x $ is zero on its support, it is identically zero, hence $ i(A) $ is linearly independent. This proves that $ i(A) $ is a basis for $ F^{\oplus A} $, which implies $ F^{\oplus A} $ is a free module on $ A \cong i(A) $. Let us consider the universal property for free objects to see whether $ F^{\oplus A} $ is free in the category $ R $-\cat{Mod}.
\end{proof}

\begin{theorem}
The $ R $-module $ F^{\oplus A} $ satisfies the universal property of a free object.
\end{theorem}

\begin{proof}
The injective set-map $ i : A \to F^{\oplus A} $ serves as our embedding of the basis. Let $ M $ be another module, and let $ f : A \to M $ be a set-map. The obvious choice for an extension morphism is to define $ \bar{f} : \sum x_i a_i \mapsto \sum x_i f(a_i) $. This map is well-defined by uniqueness of the sum in $ F^{\oplus A} $. Given two elements $ x = \sum_{i = 1}^n x_i a_i $ and $ y = \sum_{i = \ell}^m y_i a_i $, let $ N = \max(n, m) $. We extend both sums by $ y_i = 0 $ for any $ i < \ell$ and $ i > m $; $ x_i = 0 $ for any $ i > n $. Then we can write $ x = \sum_{i = 1}^N x_i a_i $ and $ y = \sum_{i = 1}^N y_i a_i $. By addition in $ F^{\oplus A} $, for one coordinate $x_i, y_i$ of $ x, y $, we see that 
\begin{align*}
	(x_i a_i + y_i a_i)(a_i) &= (x_i a_i)(a_i) + (y_i a_i)(a_i)\\
	&= x_i + y_i\\
	&= (x_i + y_i) a_i(a_i).
\end{align*}
Thus, $ (x_i + y_i)a_i = x_i a_i + y_i a_i $, proving module elements indeed distribute (This is a part of the verification that $ F^{\oplus A} $ is a module). Thus, $ x + y = \sum_{i = 1}^N (x_i + y_i) a_i $, as desired. This allows us to consider 
\begin{align*}
	\bar{f}(x + y) &= \bar{f}(\sum x_i a_i + \sum y_i a_i)\\
	&= \bar{f}(\sum (x_i + y_i) a_i)\\
	&= \sum (x_i + y_i) f(a_i)\\
	&= \sum x_i f(a_i) + y_i f(a_i)\\
	&= \sum x_i f(a_i) + \sum y_i f(a_i)\\
	&= \bar{f}(x) + \bar{f}(y).
\end{align*}
Similarly, $ \bar{f}(c x) = c \bar{f}(x) $. If we had another such morphism $ g : F^{\oplus A} \to M $ then by linearity $ g(\sum x_i a_i) = \sum g(x_i a_i) = \sum x_i g(a_i) = \sum x_i f(a_i) = \bar{f}(\sum x_i a_i) $.
\end{proof}

This shows that $ F^{\oplus A} $ is indeed the free object for the category of left modules. As we have showin in the construction, the identification of $ A $ in $ F^{\oplus A} $ is a basis for $ F^{\oplus A} $, which shows $ F^{\oplus A} $ is free on $ A $. Another way to show that $ F^{\oplus A} $ is free is if we demonstrate the above theorem for any arbitrary free $ R $-module.

\begin{theorem}
Any $ R $-module with basis $ A $ also satisfies the universal property of a free object.
\end{theorem}

\begin{proof}
Let $ i : A \to F $ be the inclusion map, and let $ f : A \to M $ be a map of sets. Define the map $ \bar{f}: F \to M $ by $ x = \sum x_i a_i  \mapsto \sum x_i f(a_i)$, where $ a_i \in A $. This map is well-defined since for $ x = y $, we have $ x_i = y_i $ for all $ i $ by uniqueness of form induced by the basis $ A $; hence $ \bar{f}(x) = \bar{f}(y) $. Similar to the previous proof, we have $ \bar{f}(x + y) = \bar{f}(\sum (x_i + y_i) a_i) = \sum (x_i + y_i) f(a_i) = \sum x_i f(a_i) + \sum y_i f(a_i) = \bar{f}(x) + \bar{f}(y) $. Also $ \bar{f}(cx) = c \bar{f}(x) $, proving $ \bar{f} $ is linear. Suppose $ g : F^{\oplus A} \to M $ is another such morphism. Then by linearity, $ g(\sum x_i a_i) = \sum x_i g(a_i) = \sum x_i f(a_i) = \bar{f}(x) $.
\end{proof}

If we only prove the above version of the theorem, then it follows by categorical considerations that $ F^{\oplus A} $ is both free and is isomorphic to any other $ R $-module that is free on $ A $.


\begin{corollary}
If we let $ A $ be a set and $ R = \mathbb{Z} $, then $ F^{\oplus A} $ is the free abelian group on $ A $.
\end{corollary}

\begin{proof}
This will follow if we show that $ F^{\oplus A} $ is also free in $ \cat{Ab} $. Indeed, by repeating the above proof of the universal property with $ R = \mathbb{Z} $, the statement is proved.
\end{proof}

\begin{theorem}
If $ R $ is a commutative ring, then the number of basis elements for the free $ R $-module is independent of the choice of basis.
\end{theorem}

\section{Tensor Products of Modules Over Commutative Rings}

\subsection{Construction and Basic Properties}

In this section we consider modules over commutative rings since many of the properties are simplified. We define the tensor product for $ R $-modules as an $ R $-module satisfying a certain universal property, after which we outline some basic properties of tensor products.

\begin{definition}
Let $ M, N $, and $ T $ be $ R $-modules. A bilinear map $ f: M \times N \to T $ is a function satisfying the following properties:
\begin{enumerate}
\item $f(m + m', n) = f(m, n) + f(m', n)$
\item $ f(m, n + n') = f(m, n) + f(m, n') $
\item $ f(rm, n) = r f(m, n) $
\item $ f(m, rn) = r f(m, n) $
\end{enumerate}
In the event that we want to emphasize the ring, we say $ f $ is $ R$-bilinear.
\end{definition}

\begin{definition}
Let $ M$ and $N $ be $ R $-modules. A tensor product of $ M $ and $ N $ is an $ R $-module $ M \tensor[R] N $ together with the $ R $-bilinear map $ \tau : M \times N \to M \tensor[R] N $ satisfying the property that, for any $ R $-bilinear map $ \varphi: M \times N \to X $, $ \varphi $ there is a unique $ R $-linear map $ \Phi: M \tensor[R] N  \to X$ such that $ \Phi \circ \tau = \varphi $.

This can be represented by the following commutative diagram, where $ \varphi, \tau $ are $ R $-bilinear and $ \Phi $ is $ R $-linear.
\[ \begin{tikzcd}
	M \times N \arrow[dr, "\varphi"'] \arrow[r, "\tau"] & M \tensor[R] N \arrow[d, dashed, "\Phi"]\\
	& X
\end{tikzcd} \]
\end{definition}

\begin{corollary}
Tensor products are unique up to unique isomorphism.
\end{corollary}

\begin{proof}
After setting up a suitable arrow category, the definition above implies that the tensor product is a terminal object in that category. In this case, it is easy to see that the two tensor products are isomorphic.
\end{proof}

\begin{theorem}
Tensor products exist in $ R $-\cat{Mod}.
\end{theorem}

\begin{proof}
We proceed with the construction of a tensor product by first considering the free $ R $-module on $ M \times N $. Let $ \iota : M \times N \to F $ be the basis map, and $ F $ the free module. In order to enforce relations in $ F $ similar to bilinearity, we let $ G $ be the submodule generated by all elements of $ F $ of the form
\begin{align*}
	\iota (m + m', n) - \iota (m, n) - \iota(m', n)\\
	\iota (m, n + n') - \iota(m, n) - \iota(m, n')\\
	\iota(rm, n) - r \iota(m, n)\\
	\iota(m, rn) - r \iota(m, n)
\end{align*}
for all $ (m, n), (m', n') \in M \times N $ and $ r \in R $. So elements of $ G $ are finite linear combinations of the above generators. If we consider an element $ \iota (m + m', n) - \iota(m, n) - \iota(m', n) \in G$, projecting this onto the quotient $ F / G $ yields
\begin{align*}
	\iota (m + m', n) - \iota(m, n) - \iota(m', n) + G = 0 + G\\
	\implies  \iota (m + m', n) - \iota(m, n) - \iota(m', n) \equiv 0 &\mod G\\
	\iota(m + m', n) \equiv \iota(m, n) + \iota(m', n) &\mod G.
\end{align*}
The other relations hold similarly. This is not surprising, seeing as we enforced this via the submodule $ G $. From these relations, one can readily see that $ \pi \circ \iota $ is bilinear. Indeed,

\begin{align*}
	\pi(\iota(m + m', n)) &= \iota(m + m', n) + G\\
	&= \iota(m, n) + \iota(m', n) + G\\
	&= \pi(\iota(m, n)) + \pi(\iota(m', n)).
\end{align*}
Again, the other properties follow similarly.

We need to show that the resulting quotient $ F / G $ satisfies the properties of the definition. Let $ \varphi : M \times N \to X $ be an $ R $-bilinear map, where $ X $ is an $ R $-module. By the universal property of free modules (here we forget about bilinearity and only consider $ \iota, \varphi $ as set maps), there is a unique $ R $-linear map $ \psi : F \to X $ making 

\[ \begin{tikzcd}
	M \times N \arrow[r, "\iota"] \arrow[dr, "\varphi"'] & F \arrow[d, dashed, "\psi"]\\
	& X
\end{tikzcd} \]
commute. We consider the natural projection $ \pi : F \to F / G $ and extend the diagram:

\[ \begin{tikzcd}
	M \times N \arrow[r, "\iota"] \arrow[dr, "\varphi"'] & F \arrow[d, dashed, "\psi"] \arrow[r, "\pi"]  & F / G\\
	& X &
\end{tikzcd} \]
Since $ \psi $ is $ R $-linear, there is a unique $ R $-linear map $ \Phi: F / G \to X $, from which $ \psi $ factors through $ F / G $ (this is a result from group theory) in the following manner.

\[ \begin{tikzcd}
	M \times N \arrow[r, "\iota"] \arrow[dr, "\varphi"'] & F \arrow[d, dashed, "\psi"] \arrow[r, "\pi"]  & F / G \arrow[dl, dashed, "\Phi"]\\
	& X &
\end{tikzcd} \]
We have the equations $ \psi \circ \iota = \varphi $ and  $\Phi \circ \pi = \psi $, which allows us to deduce
\begin{align*}
	\Phi \circ \pi \circ \iota &= (\Phi \circ \pi) \circ \iota\\
	&= \psi \circ \iota\\
	&= \varphi
\end{align*}
making the diagram above commute. As we've previously shown, $ \tau := \pi \circ \iota $ is $ R $-bilinear. It also follows that $ \Phi $ is unique, as a result of the unique factorization of $ \psi $ through $ F / G $. To be careful, if we have another such $ \Phi' : F / G \to X $, then $ \Phi' \circ \pi \circ \iota = \varphi = \psi \circ \iota$. Since $ \iota(M \times N) $ generates $ F $, it follows that $ \Phi' \circ \pi = \psi = \Phi \circ \pi$. Surjectivity of $ \pi $ means that it's an epimorphism (right-cancellative), hence $ \Phi' = \Phi $, as desired.
\end{proof}

We denote the image of $ \tau : M \times N \to M \tensor[R] N $ by $ \tau(m, n) = m \tensor n $. Every element in the image of $ \tau $ is called a simple or monomial tensor, whose span we will see generates the entire tensor product.

\begin{proposition}
The simple tensors $ m \tensor n $ generate $ M \tensor[R] N $.
\end{proposition}

\begin{proof}
Let $ \tau : M \times N \to M \tensor[R] N $ be the canonical bilinear map, and let $ T  = R $ $\tau (M \times N)$ be the submodule of $ M \tensor[R] N $ generated by the simple tensors. Let $ z : M \times N \to (M \tensor[R] N) / T $ be the zero map, and consider the natural projection $ \pi : M \tensor[R] N \to (M \tensor[R] N) / T $. It is clear that $ z $ is bilinear, so by the universal property $ \pi $ is our unique linear map. 

\[ \begin{tikzcd}
	M \times N \arrow[r, "\tau"] \arrow[dr, "z"'] & M \tensor[R] N \arrow[d, "\pi = 0"]\\
	& (M \tensor[R] N) / T
\end{tikzcd}\]

The commutativity of the diagram guarantees that $ \pi $ is zero on the image of $ \tau $, i.e., $ \pi \circ \tau = z = 0 $. However, the zero map from $ M \tensor[R] N  $ to $ (M \tensor[R] N) / T $ also satisfies the universal property, hence $ \pi = z = 0 $. That is, for each $ x \in M \tensor[R] N $ $ \pi(x) = 0 + T $, implying that $ x \in T $ and  $ T = M \tensor[R] N $.
\end{proof} 

\begin{proposition}
Let $ M, N, $ and $ T $ be $ R $-modules.
\begin{enumerate}
\item $ M \tensor N \cong N \tensor M $
\item $ M \tensor (N \tensor L) \cong (M \tensor N) \tensor L $
\end{enumerate}
\end{proposition}

\begin{proof}
($ 1 $) Let $ f : M \times N \to N \times M $ be the flip $ (m, n) \mapsto (n, m) $. It is clear that $ f $ is bijective, so $ \inv{f} : N \times M \to M \times N $ exists. The universal property gives us the following diagram
\[ \begin{tikzcd}
	M \times N \arrow[d, shift right = 2, "f"'] \arrow[r, "\tau_1"] & M \tensor N \arrow[d, shift right = 2, "\phi"']\\
	N \times M \arrow[u, shift right = 2, "\inv{f}"'] \arrow[r, "\tau_2"] & N \tensor M \arrow[u, shift right = 2, "\psi"']
\end{tikzcd}\]
where $ \phi $ and $ \psi $ are the unique linear maps satisfying the universal property in their respective direction. It is straightforward to show that $ \tau_2 \circ f$ and $ \tau_1 \circ \inv{f} $ are bilinear, which is inherited from the bilinearity of $ \tau_1, \tau_2 $. All that is left is to show that $ \phi $ and $ \psi $ are inverses of eachother. First observe that $ \phi \circ \tau_1 = \tau_2 \circ f $, which means $ \phi(m \tensor n) = n \tensor m $. Similarly, we also have $ \psi (n \tensor m ) = m \tensor n $. Then
\begin{align*}
	\phi(\psi(n \tensor m)) &= \phi(m \tensor n)\\
	&= n \tensor m,\\
	\psi(\phi(m \tensor n)) &= \psi(n \tensor m)\\
	&= m \tensor n,
\end{align*}
which implies $ \phi \circ \psi = 1_{N \tensor M} $ and $ \psi \circ \phi = 1_{M \tensor N} $. Since these maps agree on generating sets, it follows that they agree everywhere. An application of Proposition \ref{concrete_gen} guarantees this.

($ 2 $) This is slightly more complicated to show since we do not immediately have a canonical bilinear map from $ (M \times N) \times L $ to $ (M \tensor N) \tensor L $. We will see that the canonical bilinear map is actually trilinear. Consider $ \tau_1 \times 1_L : (M \times N) \times L \to (M \tensor N) \times L $, where $ \tau_1 : M \times N \to M \tensor N $ is the canonical bilinear map, and $ 1_L : L \to L $ is the identity. Next, let $ \tau_2 : (M \tensor N) \times L \to (M \tensor N) \tensor L $ be the corresponding canonical bilinear map. The map $ \tau = \tau_2 \circ (\tau_1 \times 1_L) : (M \times N) \times L \to (M \tensor N) \tensor L $ is linear in each argument, hence is also bilinear in $ M \times N $ and $ L $. Similarly, we have a map $ \sigma = \sigma_2 \circ (1_M \times \sigma_1) : M \times (N \times L) \to M \tensor (N \tensor L) $, where $ \sigma_1: N \times L \to N \tensor L $ is the corresponding canonical bilinear map and $ \sigma_2 : M \times (N \tensor L) \to M \tensor (N \tensor L) $ is its corresponding canonical bilinear map. Let $ f : (M \times N) \times L \to M \times (N \times L) $ be given by $ ((m, n), \ell) \mapsto (m, (n, \ell)) $; $ f $ is clearly bijective. Now we have the diagram
\[ \begin{tikzcd} 
	(M \times N) \times L \arrow[d, "f"] \arrow[r, "\tau_1 \times 1_L"] & (M \tensor N) \times L \arrow[r, "\tau_2"] & (M \tensor N) \tensor L \\
	M \times (N \times L) \arrow[r, "1_M \times \sigma_1"] & M \times (N \tensor L) \arrow[r, "\sigma_2"] & M \tensor (N \tensor L)
\end{tikzcd}
\]

***messy, messy, and I don't think this will work
\end{proof}

\begin{theorem}
Let $ f : M \to M' $ and $ g : N \to N' $ be linear maps of $ R $-modules. There is a unique linear map $ h : M \tensor N \to M' \tensor N' $ such that $ h(m \tensor n) = f(m) \tensor g(n) $ for all $ (m, n) \in M \times N $. This map is denoted $ f \tensor g $.
\end{theorem}

\begin{proof}
Consider the following diagram
\[ \begin{tikzcd}
	M \times N \arrow[d, "f \times g"] \arrow[r, "\tau"] & M \tensor N \arrow[d, dashed, "\phi"]\\
	M' \times N' \arrow[r, "\tau'"] & M' \tensor N';
\end{tikzcd}\]
It can be seen that $ \tau' \circ (f \times g) : M \times N \to M' \times N'$ is bilinear, which implies there is a unique linear map $ \phi : M \tensor N \to M' \tensor N' $ making the diagram commute. In particular, we have $ \phi(m \tensor n) = \phi(\tau(m, n)) = (\tau' \circ f \times g)(m, n) = \tau'(f(m), g(n)) = f(m) \tensor g(n)$. For this reason, we denote $ \phi = f \tensor g $.
\end{proof}

\begin{proposition}
If $ f $ and $ f' $ (respectively, $ g $ and $ g' $) are composible linear maps then 
\[(f \circ f') \tensor (g \circ g') = (f \tensor g) \circ (f' \tensor g'). \]
\end{proposition}

\section{Linear Maps Between Vector Spaces}

\begin{remark}
Unless otherwise stated, all vector spaces are assumed to be finite-dimensional. We also assume the reader is familiar with basic results of linearly independent sets, spanning sets, and basis sets. Such results include spanning sets have $ \geq $ cardinality than linearly independent sets, dimension is invariant, etc.
\end{remark}

\begin{definition}
A linear map $ T : V \to W $ is called \textit{invertible} or \textit{nonsingular} if it is bijective as a set map. It is called \textit{singular} if it is not invertible.
\end{definition}

The following theorem is a general result describing how a linear map decomposes. This can be reproduced in many similar settings for groups, rings, modules, etc.

\begin{theorem} \label{decomp}
If $ V$ and $W $ are vector spaces and $ T \in Hom(V, W) $, then $ T $ factors uniquely through $ V / \ker T $ such that the following diagram commutes.
\[ \begin{tikzcd}
	V \arrow[dr, two heads, "\pi"] \arrow[rr, "T"] & & W\\
	& V / \ker T \arrow[ur, hook, dashed, "\bar{T}"] &
\end{tikzcd}\]
\end{theorem}

\begin{proof}
Indeed, the canonical projection $ \pi : V \to V / \ker T $ defined by $ v \mapsto v + \ker T $ is linear and surjective. Define $ \bar{T}: V / \ker T \to W $ given by $ v + \ker T \mapsto T(v) $. If $ v_1 + \ker T = v_2 + \ker T $, then $ v_1 - v_2 \in \ker T $ and $ T(v_1 - v_2) = T(v_1) - T(v_2) = 0 $, meaning $ \bar{T} $ is well-defined. Linearity of $ T $ implies that $ \bar{T} \in \Hom(V / \ker T, W) $. It is also clear that $ \bar{T} $ has trivial kernel, since we modded out the kernel of $ T $. However, let us see this explicitly. Let $ x + \ker T \in \ker \bar{T} $. Then $ \bar{T}(x + \ker T) = T(x) = 0 $, meaning $ x \in \ker T $ and hence $ x + \ker T = 0 + \ker T $. Thus, $ \ker \bar{T} = 0 $. This proves that $ T $ is injective. Moreover, any such map is necessarily $ \bar{T} $ due to commutativity of the diagram.
\end{proof}

\begin{corollary}[The First Isomorphism Theorem] \label{iso1}
Let $ T : V \to W $ be a linear map. Then $ V / \ker T \cong \im T $.
\end{corollary}

\begin{corollary} \label{iso2}
Let $ X$ and $ Y $ be subspaces of $ V $. Then
\[ \frac{X+ Y}{X} \cong \frac{Y}{X \cap Y}. \]
\end{corollary}

\begin{proof}
Let $ \iota : Y \to X + Y $ be the inclusion map, and let $ \pi : X + Y \to \frac{X+ Y}{X} $ be the canonical projection. We have the commuting diagram

\[\begin{tikzcd}
	Y \arrow[dr, "T"'] \arrow[r, "\iota"] & X + Y \arrow[d, "\pi"]\\
						 & \frac{X+Y}{X}
\end{tikzcd}\]
where $ T = \pi \circ \iota $. It can be readily shown that $ \ker T = X \cap Y $. Indeed, 
\begin{align*}
	y \in \ker T \subset Y &\iff T(y) = y + X = 0 + X\\
	&\iff y \in X \cap Y.
\end{align*}
By Corollary \ref{iso1} it follows that $ Y / (X \cap Y) \cong (X + Y) / X $.
\end{proof}

\begin{corollary} \label{inj_eqv}
Let $ V $ and $ W $ be arbitrary vector spaces. Then $ T : V \to W $ is injective iff $ \ker T = 0 $.
\end{corollary}

\begin{proof}
If $ T $ is injective, then any $ x \in \ker T $ satisfies $ T(x) = 0 = T(0) $, hence $ x = 0 $. Conversely, if $ \ker T = 0 $ then any $ T(x) = T(y) \implies T(x - y) = 0$. Thus $ x - y \in \ker T = 0 $ so $ x = y $.
\end{proof}

\begin{proposition} \label{indp_inj}
Let $ T : V \to W $ be a linear map and let $ e_1, \ldots, e_n $ be a basis of $ V $. If $ T(e_1), \ldots, T(e_n) $ are linearly independent in $ W $, then $ T $ is injective.
\end{proposition}

\begin{proof}
Let $ v = \sum c_i e_i $ and $ v' = \sum d_i e_i $ be vectors in $ V $. If $ T(v) = T(v') $ then $ \sum c_i T(e_i) = \sum d_i T(e_i) $. By linear independence of the basis vectors sent to $ W $ by $ T $, we have $ c_i = d_i $ for all $ i $. Thus, $ v = v' $.
\end{proof}

\begin{proposition} \label{inj_indp}
Let $ V $ and $ W $ be any arbitrary vector spaces. If $ T : V \to W $ is injective and $ (v_i)_1^k $ is a linearly independent set of vectors in $ V $, then $ (T(v_i))_i^k $ is linearly independent in $ W $.
\end{proposition}

\begin{proof}
It is easy to see that if $ \sum c_i T(v_i) = 0 $, then $ T( \sum c_i v_i) = 0 = T(0)$, hence $ \sum c_i v_i = 0 $. By linear independence in $ V $, it follows that $c_i = 0 $ for all $ i $.
\end{proof}

\begin{example}
Consider the $ \R $-vector space $ C^\infty(\R) $ of smooth $ \R $-valued functions where addition and scalar multiplication is defined pointwise. The differentiation map $ D : C^\infty(\R) \to C^\infty(\R) $ is a linear operator, so we may consider applying Proposition \ref{inj_indp}, which would give us an interesting way to show that functions are linearly independent. The problem is that $ D $ is not exactly injective, since $ D(0) = D(1) = 0 $; its kernel is nontrivial. Let's try to make $ D $ injective by modding out by its kernel. Theorem $ \ref{decomp} $ would give a factorization of the differentiation operator whose behaviour should be similar to the original. We must first compute the kernel. Indeed, if $ f \in \ker D $, then $ D(f) = 0 $ meaning $ f $ is a constant function. Identifying the subspace of constant functions as $ \R \subset C^\infty(\R) $, we may continue as planned.

By the decomposition theorem, we have an injective linear map $ \bar{D} : C^\infty(\R) / \R \to C^\infty(\R) $ whose image coincides with that of $ D $. Functions $ f, g \in C^\infty(\R) $ are considered to be equal if they differ by a constant. That is, $ f \equiv g \mod \R \iff f -g \in \R \iff f - g = c$ for some constant $ c \in \R $. We lose some information in the process above, but in certain applications this is just fine. In the study of differential equations, usually one is interested in the general solution of a particular differential equation. For simplicity we consider ordinary differential equations. A useful result is one that is known as the "superposition principle". It states that if $ f_1, \ldots, f_n $ are linearly independent solutions to a differential equation, then any linear combination of those solutions is also a solution. This just means that the solution space of a differential equation is a vector space itself. It is easy to see this, especially since $ D $ is a linear operator.

The map $ \phi \in \End(C^\infty(\R)) $ given by $ y \mapsto (a_n(x)D^n + \ldots + a_1(x)D + a_0(x))y $ is called an $ n $th order linear differential equation. Consider the set of homogeneous solutions to $ \phi $. That is, smooth functions $ f $ such that $ \phi(f) = 0 $. Clearly we're talking about the kernel, $ \ker \phi $. Immediately we get that the set of homogeneous solutions form a subspace of $ C^\infty(\R) $, so linear combinations of existing homogeneous solutions also yields a homogeneous solution. It can be shown that the number of linearly independent solutions is equal to the order of the differential equation. 


\end{example}

We will use the following lemma in a few places when we need to expand our sets of vectors. The proof is straightforward but tedious to show, so we exclude it.

\begin{lemma} \label{extend}
Any non-maximal linearly independent set of vectors in a finite-dimensional vector space can be extended to a basis.
\end{lemma}

\begin{corollary} \label{inj_dim}
If $ T : V \to W $ is injective, then $ \dim V \leq \dim W $.
\end{corollary}

\begin{proof}
If $ (v_i)_1^n $ is a basis for $ V $, then $ (T(v_i))_1^n $ is linearly independent in $ W $. Extending it to a basis of $ W $ via Lemma \ref{extend} yields the result.
\end{proof}

\begin{corollary}
If $ V $ and $ W $ are vector spaces such that $ V \subset W $, then $ \dim V \leq \dim W $.
\end{corollary}

\begin{proof}
Consider the injective inclusion map $ \iota (v) = v $ and apply Corollary \ref{inj_dim}
\end{proof}

Propositions \ref{indp_inj} and \ref{inj_indp} allow us to view injective linear maps as those that preserve linear independence. This has a few useful consequences as we give in the following example.

\begin{example}

\end{example}

\begin{proposition}
For any subspace $ U $ of $ V $, there exists a linear map $ T : V \to W $ such that $ U = \ker T $
\end{proposition}

\begin{proof}
Let $ \mathcal{B} = \{ v_1, \ldots, v_n \} $ be a basis of $ V $ such that $ v_1, \ldots, v_{\ell} $ is a basis of $ U $. Then $ V $ is free on $ \mathcal{B} $, so we have the following commuting diagram
\[\begin{tikzcd} 
	\mathcal{B} \arrow[dr, "j"'] \arrow[r, "\iota"] & V \arrow[d, dashed, "T"]\\
	& V
\end{tikzcd}\]
where $ \iota $ is inclusion, $ T $ is the unique linear map making the diagram commute, and
\[ j(v_i) = 
\begin{cases}
	v_i, &i > \ell\\
	0, &i \leq \ell
\end{cases}.\]
Then $ T(U) = 0 $ and if $ v \in \ker T $ we write $ v = \sum c_i v_i $ so that $ T(v) = c_1 T(v_1) + \ldots + c_n T(v_n) = c_{\ell + 1} T(v_{\ell+1}) + \ldots + c_n T(v_n) = 0 $ implies $ c_i = 0 $ for all $ i > \ell $ since $ T $ is the identity on these basis vectors. Thus, $ v = c_1 v_1 + \ldots c_\ell v_\ell $, proving $ U = \ker T $.
\end{proof}

\begin{lemma}
Let $ U$ be a subspace of V. There is a subspace $ W $ of $ V $ such that $ V = U \oplus W $.
\end{lemma}

\begin{proof}
Extend the basis $ (x_i)_1^\ell $ of $U $ to a basis $ (x_i)_1^n $ of $ V $. If we let $ W = \vspan (x_i)_{\ell+1}^k $ we will see that $ W $ is a subspace satisfying the lemma. Certainly $ V = U + W $, so we need only show the two subspaces meet trivially. Let $ x \in U \cap W $ and write $ x = \sum_{i=1}^\ell c_i x_i = \sum_{i=\ell + 1}^n c_i x_i $. It follows that 
\begin{align*}
	\sum_{i=1}^\ell c_i x_i - \sum_{i=\ell + 1}^n c_i x_i &= 0\\
	\sum_{i = 1}^n c_i x_i &= 0,
\end{align*}
hence $ c_i = 0 $ for all $ i $, seeing as $ (x_i)_1^n $ is linearly independent. Therefore, $ x = 0 $ which proves $ V = U \oplus W $.
\end{proof}

\begin{lemma} \label{ker_sum}
For a linear map $ T : V \to W $, $ V = \ker T \oplus U $, where $ U $ is isomorphic to $ \im T $.
\end{lemma}

\begin{proof}
We have $ V = \ker T \oplus U $ for some subspace $ U $ of $ V $ by the above corollary. Moreover, $ V / \ker T \cong \im T $ by Theorem \ref{iso1}. An application of Corollary \ref{iso2} yields
\begin{align*}
	\im T &\cong V / \ker T\\
	&= (\ker T \oplus U) / \ker T\\
	&\cong U / (\ker T \cap U)\\
	&\cong U,
\end{align*}
hence $ U \cong \im T $. Note that $ X / 0 \cong X $ for any vector space $ X $, which is used to obtain $ U / (\ker T \cap U) \cong U $.
\end{proof}

Note that the following corollary is true only when $ T $ is a projection operator. Otherwise, the support subspace of the operator will only be isomorphic, and not necessarily equal to its image.

\begin{corollary}
If $ T: V \to V $ is a linear operator then $ V = \ker T \oplus \im T $ provided that $ T^2 = T $.
\end{corollary}

\begin{proof}
Since $ T^2 = T $ we know that 
\end{proof}


\begin{theorem} [The Rank-Nullity Theorem, Fundamental Theorem of Linear Maps]
For finite-dimensional vector spaces $ V, W$ and linear map $ T : V \to W$, it follows that
$ \dim V = \dim \ker T + \dim \im T$.
\end{theorem}

\begin{proof}
By Lemma \ref{ker_sum}, $ V = \ker T \oplus U $ where $ U \cong \im T $. In particular, $ \dim U = \dim \im T $ which allows us to conclude subsequently
\begin{align*}
	\dim V &= \dim (\ker T \oplus U)\\
	&= \dim \ker T + \dim U\\
	&= \dim \ker T + \dim \im T.
\end{align*}
\end{proof}

\begin{proof}[Long proof using bases]
Let $ (x_i)_1^\ell $ be a basis for $ \ker T $ and extend it to a basis $ (x_i)_1^n$ of $ V $. We claim that $ (y_i)_1^m := (x_i)_{\ell+1}^n $ is linearly independent in $ V / \ker T $. Suppose $ (c_1 y_1 + \ldots + c_m y_m) + \ker T = 0 + \ker T $. By the lemma above, $ \vspan (y_i)_1^m \cap \ker T = 0$, so $ c_1 y_1 + \ldots + c_m y_m =0 $ and hence $ c_i =0 $ for all $ i $ by linear independence in $ V $. Therefore, $ (\pi(y_i))_1^m $ is linearly independent in $ V / \ker T $. Using the factorization theorem, there is a unique injective map $ \bar{T} : V / \ker T \to W $ which preserves linear independence of $ (\pi(y_i))_1^m $ into $ W $. Thus, $ (T(y_i))_1^m $ is linearly independent, which leaves us with proving it spans $ \im T $. Given $ w \in \im T $, $ w = T(v) $ for some $ v \in V $. We can write $ v = \sum_1^\ell c_i x_i + \sum_1^m d_i y_i$, since the basis of $ V $ splits nicely. Then
\begin{align*}
	T(v) &= T(\sum c_i x_i + \sum d_i y_i)\\
	&= \sum c_i T(x_i) + \sum d_i T(y_i)\\
	&= 0 + \sum d_i T(y_i)\\
	&= \sum d_i T(y_i),
\end{align*}
proving $ w \in \vspan (T(y_i))_1^m $. Therefore, $ \dim \im T = m = n - \ell $ and $ \dim \ker T = \ell $, hence $ \dim V = (n - \ell) + \ell = \dim \im T + \dim \ker T $.
\end{proof}

\begin{corollary} \label{rn_cor}
Let $ W $ be a subspace of $ V $. Then $ \dim V / W = \dim V - \dim W $.
\end{corollary}

\begin{proof}
Since $ W $ is a subspace of $ V $, it is the kernel of some linear map from $ V $ to $ V $. In particular, the map $ j : basis(V) \to V $ sending the basis of $ W $ to $ 0 $ and fixing rest of the basis of $ V $ is of interest. Extending this map to $ T $ by linearity, $ T \in \End(V)$ has kernel $ W $. By the isomorphism theorem we have $ V / W \cong \im T $, which, together with the rank-nullity theorem allows us to conclude: $ \dim V = \dim \ker T + \dim \im T = \dim W + \dim V / W$.
\end{proof}

\begin{corollary}
Let $ U, W $ be subspaces of $ V$. Then $ \dim(U + W) = \dim U + \dim W - \dim (U \cap W) $.
\end{corollary}

\begin{proof}
By Corollary \ref{iso2}, $ (U + W) / W \cong U / (U \cap W)$. Then by Corollary \ref{rn_cor} we have $ \dim (U + W) - \dim W = \dim U - \dim (U \cap W)$, hence $ \dim (U + W) = \dim U + \dim W - \dim (U \cap W) $.
\end{proof}

\begin{theorem}
Let $ V $ be a finite-dimensional vector space, with $ T \in End(V) $. The following are equivalent.
\begin{enumerate}
\item $ T $ is invertible.
\item $ T $ is injective.
\item $ T $ is surjective.
\end{enumerate}
\end{theorem}

\begin{proof}
$ (1) \implies (2) $ and $ (1) \implies (3) $ both follow by definition of invertible.

$ (2) \implies (3) $. If $ T $ is injective, then $ \ker T = 0 $ hence $ \dim V = \dim \ker T + \dim \im T = \dim \im T $, proving $ T $ is surjective.

$ (2) \implies (1) $. We use the same argument above.
\end{proof}


\begin{definition}
The cokernel of a linear map $ T : V \to W $ is defined as $ \coker T := W / \im T $
\end{definition}

\begin{proposition}
Let $ T : V \to W $ be a linear map.
\begin{enumerate}
\item $ T $ is injective iff $ \ker T = 0 $.
\item $ T $ is surjective iff $ \coker T = 0 $.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
\item If $ T $ is injective, then $ x \in \ker T \implies T(x) = 0 = T(0) $, hence $ x = 0 $ by injectivity of $ T $. Conversely, if $ T(x) = T(y) $ then $ T(x - y) = 0 $, hence $ x - y \in \ker T \implies x - y = 0$.

\item If $ T $ is surjective, it is immediate that $ \coker T = W / W = 0 $. Conversely, if $ \coker T = 0 $, then for any $ w \in W $, if $ w \notin W $, then $ w + \im T \neq 0 + \im T $ in $ \coker T $, which contradicts our assumption.
\end{enumerate}
\end{proof}

\begin{proposition}
Let $ T: V \to W $ be a linear map.
\begin{enumerate}
\item If $ T $ is surjective, then $ \dim V \geq \dim W $.
\item If $ T $ is injective, then $ \dim V \leq \dim W $.
\item If $ T $ is bijective, then $ \dim V = \dim W $
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
\item Since $ T $ is surjective, $ \im T = W $ which clearly means $ \dim \im T = \dim W $. The rank-nullity theorem then yields
\begin{align*}
	\dim V &= \dim \ker T + \dim \im T\\
	&= \dim \ker T + \dim W\\
	&\implies \dim W \leq \dim V.
\end{align*}

\item This was shown previously by the first isomorphism theorem, followed by extending a basis to $ W $.

\item This follows immediately from the first two.
\end{enumerate}
\end{proof}

\begin{corollary}\label{directDim}
If $ U $ and $ W $ are subspaces of $ V $ such that $ \dim U + \dim W = \dim V $, then $ V = U \oplus W $.
\end{corollary}

\begin{proof}
Let $ u_1, \ldots, u_k $ be a basis for $ U $ and extend it to a basis $ u_1, \ldots, u_k, w_1, \ldots w_\ell $ of $ V $. Then $ \dim W = \ell = \dim V - k$ by assumption of dimensions. We claim $ w_i \in W $ for all $ i $. By assumption, $ w_i \notin U $ since the first basis for $ U $ given is linearly independent and maximal. If without loss of generality, $ w_\ell \notin W $ then every $ w \in W $ can be written as a sum of less than $ \ell $ basis elements, contradicting the dimension of $ W $.

Even simpler, since $ U \cap W = 0 $, they form a direct sum $ U \oplus W $ which is a subspace of $ V $. Then $ \dim U \oplus W = \dim U + \dim W = \dim V $. A subspace whose dimension coincides with the ambient space means that the subspace is the ambient space.
\end{proof}

\begin{definition}
For any $  T \in \End(V) $, where $ V $ is a $ k $-vector space, define the \textit{polynomial homomorphism} in $ T $ as $ P(T): k[x] \to \End(V) $ by $ x \mapsto T $.
\end{definition}

\begin{proposition}
Let $ V $ be a vector space and $ S, T \in \End (V) $. If there is an operator $ D \in \End(V) $ such that $ S, T \in \im P(D) $, then $ ST = TS $.
\end{proposition}

\begin{proof}
Since $ S, T \in \im P(D) $, let $ s(x), t(x) \in k[x] $ for which $ s(x) \mapsto S $ and $ t(x) \mapsto T $. Both $ \End(V) $ and $ k[x] $ are $ k $-algebras, and $ P(D) $ is an algebra homomorphism, so 
\[P(D)(s(x) t(x)) = P(D)(s(x)) P(D)(t(x)).\]
Commutativity of $ k[x] $ implies $ s(x)t(x) = t(x)s(x) $, hence 
\begin{align*}
	S \circ T &= P(D)(s(x)t(x))\\
	&= P(D)(t(x)s(x))\\
	&= P(D)(t(x)) P(D)(s(x))\\
	&= T \circ S,
\end{align*}
which proves $ ST = TS $.
\end{proof}

\begin{lemma}
If $ST = TS$ then $\ker S \cup \ker T \subset \ker ST$. ($\ker S \oplus \ker T = \ker ST$ when dimensions work out?).
\end{lemma}

\begin{proof}
TODO
\end{proof}

\begin{example}
As an example, consider the application to solving $ n $th order homogeneous differential equations with constant coefficients. Let $ \phi \in \End(C^\infty)$ be a monic differential equation given by $ \phi = D^n + \ldots + a_1 D + a_0 $, where $ D \in \End(C^\infty) $ is the differential operator. In order to find $ \ker \phi $, we may factor $ \phi $ into a product of irreducible polynomials. To do so, let us assume $ k $ is algebraically closed and write 
\[\phi = \prod_{i=1}^n (D - \lambda_i).\]
To compute $ \ker \phi $ all we must do is consider solutions to $ (D - \lambda_1) \cdots (D - \lambda_n) y = 0 $, where $ y \in C^\infty $. Since each factor of $ \phi $ is in the image of the polynomial homomorphism in $ D $, all the factors commute, hence $ y \in \ker (D - \lambda_i) $ for some $ i = 1, \ldots, n $. It is easy to see that $ \ker (D - \lambda_i) = \vspan(e^{\lambda_i x})$, so the kernel of $ \phi $ consists of a linear combination of exponential functions.
\end{example}

\begin{definition}
A linear operator $ \pi : V \to V $ is called a projection if $\pi^2 = \pi$.
\end{definition}

\begin{definition}
In a category $ \cat{C} $,  a monomorphism $ m : A \to B $ is said to split if there exists a morphism $r: B \to A $ such that $ r \circ m = \id_A $. In this case, 
\begin{enumerate}
\item $ A $ is a \textit{retract} of $ B $,
\item $ r $ is a \textit{retraction} of $ m $, 
\item $ m $ is a \textit{section} of $ r $,
\item $ (r, m) $ is a \textit{splitting} of the idempotent $ m \circ r $.
\end{enumerate}
\end{definition}

\begin{lemma} \label{linear_map_extension}
Let $ U $ be a subspace of $ V $ and let $ S : U \to W $ be a linear map. Then there exists a linear map $ T : V \to W $ such that $ \im T = \im S $ and $ T\restrict{_U} = S $.
\end{lemma}

\begin{proof}
Let $ \mathscr{U} $ be a basis of $ U $ and extend $ \mathscr{U} $ to a basis $ \mathscr{V} $ of $ V $. Let $ T : V \to W $ be the extension by linearity of the map $ t : \mathscr{V} \to W $ defined by 
\[ t(v_i) = \begin{cases}
	S(v_i), & v_i \in \mathscr{U}\\
	0, & v_i \notin \mathscr{U}
\end{cases}. \]
It is clear now this is the map such that $ \im T = \im S$ and whose restriction is just $ S $.
\end{proof}

TODO: Revise proposition to lead into splitting lemma more naturally.
\begin{proposition}
Any injective linear map $ T : V \hookrightarrow W $ is split, and induces a projection $ \pi : W \to W $.
\end{proposition}

\begin{proof}
Certainly in $ \cat{Vect}_k $ monomorphisms are equivalent to injective maps, so this is a valid statement. Moreover, this situation is not difficult to imagine since 
we may consider $ T : V \to \im T $ as invertible, so its inverse $ \inv{T} : \im T \to V $ exists. Extend $ \inv{T} $ to $ S : W \to V$ by Lemma \ref{linear_map_extension}. Then 
\begin{align*}
	(ST)(v) &= S(T(v))\\
	&= \inv{T}(T(v))\\
	&= v,
\end{align*}
hence $ S\circ T = \id_V $, proving $ V $ is a retract of $ W $. Let $ \pi = T \circ S $ and observe that 
\begin{align*}
	 \pi^2 &= (TS)^2 = TS TS\\
	 &= T (ST) S = T \id_V S\\
	 &= TS = \pi.
\end{align*}
Thus we have the following short exact sequence
\[\begin{tikzcd}
	0 \arrow[r, ""] & V \arrow[r, "T"] & W \arrow[r, "S"] & V \arrow[r, ""] & 0
\end{tikzcd}.\]
\end{proof}

\section*{Exercises}

\begin{enumerate}
\item Let $ V $ be a finite-dimensional vector space. If $ \pi \in \End(V) $ is such that $ \pi^2 = \pi $, then $ V = \ker \pi \oplus \im \pi $.

\begin{solution}
Let $ x \in \ker \pi \cap \im \pi $ so that $ \pi (x) = 0 $ and $ x = \pi(v) $ for some $ v \in V $. Then $ \pi (x) = \pi (\pi(v)) = \pi(v) = x = 0 $, hence the two subspaces intersect trivially. By the rank-nullity theorem, we have $ \dim V = \dim \ker T + \dim \im T $, which, together with Corollary (\ref{directDim}) implies $ V = \ker \pi \oplus \im \pi $.
\end{solution}

\item Let $ V $ be a vector space of finite dimension, $ n $. If $ \varphi $ is any linear endomorphism on $ V $, prove there is an integer $ m $ such that the intersection of the image of $ \varphi^m $ and the kernel of $ \varphi^m $ is $ 0 $.

\begin{solution}
Intuitively, the image gets smaller as the map is applied, so we obtain a decreasing sequence
\[ \im T \supset \im T^2 \supset \ldots, \]
which must stabilize eventually, i.e., $ \im T^k = \im T^{k+1} $ for some $ k $. This follows since $ \im T^\ell $ is bounded below by $ 0 $. Let $ m $ be such that $ \im T^m = \im T^{m+1} $, and let $ \pi = T^m $. 	
\end{solution}

\item Let $ \varphi \in End(V) $ such that $ \varphi^2 = 0 $. Prove that the image of $ \varphi $ is contained in the kernel of $ \varphi $ and hence that the rank of $ \varphi $ is at most $ \frac{n}{2} $.

\begin{solution}

Let $ n = \dim V $, $ k = \dim \ker \varphi $, and $ m = \dim \im \varphi $. By assumption $ \varphi^2 = 0 $ implies that $ \im \varphi \subset \ker \varphi $, hence $ m \leq k $. The rank-nullity theorem yields 	$n = k + m \geq m + m = 2m$, hence $ m \leq \frac{n}{2} $.
\end{solution}
 
\item Let $ T : V \to W $ be a surjection of vector spaces, where $ W $ is finite-dimensional. Show that $ T $ has a right inverse.
\begin{solution}
	Let $ w_1, \ldots, w_\ell $ be a basis of $ W $. Then $ T(v_i) = w_i $ for one of possibly many such choices of $ v_i \in V $ for each $ i = 1, \ldots, \ell $. Define the map $ S : W \to V $ given by mapping $ w_i $ to $ v_i $, then extending by linearity. Thus, for $ w = \sum a_i w_i \in W $, we have
	\begin{align*}
		T \circ W(w) &= T(S(\sum a_i w_i))\\
		&= T(\sum a_i S(w_i))\\
		&= \sum a_i T(v_i)\\
		&= \sum a_i w_i = w.
	\end{align*}
\end{solution}

\item Let 
\[ 0 \to V_1 \to V \to V_2 \to 0 \]
be a short exact sequence of vector spaces, assuming $ V_2 $ is finite-dimensional. Show that it admits a splitting.
\begin{solution}
Let $ T : V_1 \to V $ and $ S : V \to V_2 $ be the above linear maps. Then $ T $ is injective and $ S $ is surjective. We also have $ \im T = \ker S $.
\begin{align*}
	\im T = \ker S \implies V / \ker S = V / \im T = \coker T \cong \im S = V_2
\end{align*}
I think $ V $ splits as $ \im T \oplus \coker T $?
\end{solution}
\end{enumerate}

\section{Realizing Linear Operators as Square Matrices}

\subsection{Connecting Linear Maps and Rectangular Matrices}

\begin{lemma}
Let $ A $ be an $ m \times n$ matrix, and let $ e_i $ be the $ i $th standard basis vector in $ F^n $. Then $ A \cdot e_\ell = (A)_{\_ \ell}$. That is, multiplying the $ \ell $th standard basis vector by a matrix on the left picks out the $ \ell $th column of that matrix.
\end{lemma}

\begin{proof}
This is a straightforward calculation
\begin{align*}
	(A e_\ell)_{i1} &= \sum_{k = 1}^n (A)_{ik} (e_\ell)_{k1}\\
	&= (A)_{i1} (e_\ell)_{11} + \ldots + (A)_{i \ell} (e_\ell)_{\ell 1} + \ldots + (A)_{i n} (e_\ell)_{n 1}\\
	&= (A)_{i 1} \cdot 0 + \ldots + 0 + (A)_{i \ell} \cdot 1 + 0 + \ldots + (A)_{i n} \cdot 0\\
	&= (A)_{i \ell}.
\end{align*}
Thus we have the column vector $ A e_\ell = (A_{1 \ell}, \ldots, A_{n \ell})$.
\end{proof}

\begin{theorem}
There is a bijection of sets
$ \Hom_R (R^n, R^m) \leftrightarrow \Mat_{n \times m}(R) $,
where $ R $ is a unital ring.
\end{theorem}

\begin{proof}
Let $ e_i $ denote the $ i $th standard basis vector, and let $ T : R^n \to R^m $ be an $ R $-module homomorphism. We produce the matrix
\[ [T] = \begin{bmatrix}
	\mid & \mid & \ldots & \mid\\
	T(e_1) & T(e_2) & \ldots & T(e_n)\\
	\mid & \mid & \ldots & \mid
\end{bmatrix}\]
which we can understand by checking where it sends the basis vectors. Recall that for a matrix $ A $, left multiplication of $ A $ on the $ i$th standard basis vector yields the $ i $th column of the matrix: $ A e_i = (A)_{-, \space j} $. Thus, multiplying $ [T] e_i = T(e_i) $ for each $ i $. Moreover, 
\begin{align*}
	[T](v) &= [T](a_1 e_1 + \ldots + a_n e_n)\\
	&= a_1 [T] e_1 + \ldots +a_n [T] e_n\\
	&= a_1 T(e_1) + \ldots + a_n T(e_n),
\end{align*}
which shows that $ [T] = T$ as maps.

Conversely, if we are given a matrix $ M \in \Mat_{n \times m}(R)$, we construct a linear map $ T : R^n \to R^m $. The idea is to map basis elements to have the same image, then extend uniquely by linearity, giving a linear map that is represented by the matrix $ M $. In particular, define $ T(e_i) = M \cdot e_i $ where $ \cdot $ on the right is matrix multiplication. 
\end{proof}

\begin{corollary}
Let $ V $ and $ W $ be vector spaces of dimensions $ m $ and $ n $, respectively. There is an isomorphism between $ \Hom(V, W) $ and $ \Mat_{n \times m} $.
\end{corollary}

\begin{proof}
The above bijection becomes linear when taking $ R = k $ the ground field.
\end{proof}

\begin{proposition}
$ \dim \Mat_{n \times m} = nm $
\end{proposition}

\begin{proof}
A basis for $ \Mat_{n \times m} $ is the set of matrices with a $ 1 $ in each entry. There are $ nm $ combinations of such matrices, hence $ \dim \Mat_{n \times m} = nm $.
\end{proof}

\begin{corollary} \label{hom_dim}
$ \dim \Hom(V, W) = (\dim V) (\dim W) $.
\end{corollary}

\subsection{Computations Regarding Linear Operators}

\begin{definition}
For a vector space $ V $ and $ \mathcal{V} = \{v_1, \ldots, v_n\} \subset V$ the span, or the subspace generated by $ \mathcal{V} $ is defined to be
\[ \vspan (V) := \left \{ \sum_{i = 1}^n c_i v_i \mid c_i \in F \right \}.\]
Recall that this is equivalent to Definition \ref{def_concrete_gen}.
\end{definition}

The subspace generated by a given set of vectors is interesting in particular especially since every vector space is free on its basis. It would be useful to determine redundancies among sets of vectors so that we can determine the minimal linearly independent set. 

\begin{example}
Consider the set of vectors $ \{ (1, 3, 4), (1, 1, 1), (-1, 0, 1) \} \subset F^3$ , which subspace is generated by these vectors? How can we determine a basis, and hence the dimension of the generated subspace?
\end{example}

In order to answer such questions, we must further develop the machinery of linear operators and how their matrix representations can be used to help simplify computations.

\begin{proposition}
If $ v_1, \ldots, v_n \in V $ and the set-map $ j: e_i \mapsto v_i $ is extended by linearity to $ T \in \End(V) $, then
\[ \vspan(v_1, \ldots, v_n) = \im T. \]
\end{proposition}

\begin{proof}
Let $ v = \sum c_i v_i \in \vspan(v_1, \ldots, v_n) $. Immediately we have that $ v = \sum c_i T(e_i) = T(\sum c_i e_i) $, since $ T $ is linear and $ T \mid_E = j $ ($ E $ is the standard basis); hence $ v \in \im T $. Conversely, given $ T(v) \in \im T $ we write $ v = \sum c_i v_i $ in terms of the basis elements so that $ T(v) = T(\sum c_i e_i) = \sum c_i T(e_i) = \sum c_i v_i $. Therefore, $ T(v) \in \vspan(v_1, \ldots, v_n) $ as desired.
\end{proof}

This proposition allows us to convert the question of calculating the span of a set of vectors into a question of calculating the image of a linear operator, and hence a square matrix. Given $ v_1, \ldots, v_n $, we can determine the subspace generated by these vectors by considering the linear map $ T $ sending $ e_i \mapsto v_i $. The span of these vectors is then the image of the corresponding linear map, whose contents are encoded by its matrix representation $ [T] $. However in this case, $ [T] = [v_1 \cdots v_n]$ since $ [T]e_i$ is the $ i $th column of the matrix.

(I got side tracked with this chapter)...

In early linear algebra classes, the general solution of a system of equations is described as the sum of a particular solution, and all of the possible solutions to the homogenous system. That is, we get a complete solution to the equation $ Ax = b $ (assuming any exist) if we can solve $ Ax = 0 $ and find one solution to $ Ax = b $ somehow. After going through a few algebra classes, this result is completely obvious upon reflection. The machinery of quotient spaces simplifies this notion to the point where there is almost nothing to prove.

\begin{proposition}[General Solution of $ Ax = b $]
Let $ T \in \End(V) $ be a linear operator, where $ \dim V < \infty $, and let $ y \in V $. A general solution to the equation $ T(x) = y $ is given by $ T^{-1}(y) = \ker T + x_p$, where $ x_p \in T^{-1}(y) $ is a known solution.
\end{proposition}

\begin{proof}
It is easy to see this following the chain of equivalences,
\begin{align*}
	x \in T^{-1}(y) &\iff T(x) = y = T(x_p)\\
	&\iff x - x_p \in \ker T\\
	&\iff x \in x_p + \ker T.
\end{align*}
To verify that this makes sense, we see that 
\begin{align*}
	T(x_p + \ker T) &= T(x_p) + T (\ker T)\\
	&= y + 0 = y.
\end{align*}
\end{proof}

\section{Dual Vector Spaces}

\subsection{Basics Properties}

\begin{definition}
A \textit{linear functional} or \textit{linear form} is a linear map from $ V $ to the ground field. An element of $ \dual{V} $ corresponding to a particular element $ v \in V $ will be denoted by $ \dual{v} $.
\end{definition}

\begin{definition}
The dual space of a vector space $ V $ is defined to be $ \dual{V} = \Hom(V, k) $.
\end{definition}

\begin{proposition}
$ \dim \dual{V} = \dim V$
\end{proposition}

\begin{proof}
By Corollary \ref{hom_dim}, $ \dim \Hom(V, k) = (\dim V) (\dim k) = \dim V$.
\end{proof}

The following requires no proof and is completely obvious, but is worth explicitly mentioning. It is also worth mentioning that this isomorphism is not canonincal.

\begin{corollary}
If $ V $ is finite-dimensional, then $ \dual{V} \cong V $.
\end{corollary}

\begin{definition}
Let $ v_1, \ldots, v_n $ be a basis of $ V $. The \textit{dual basis} corresponding to the one mentioned above is given by $ \dual{v_i} = \pi_i $, where $ \pi_i : V = \bigoplus_{j = 1}^n k v_j \to k $ is the $ i $th coordinate projection.
\end{definition}

\begin{example} \label{forms_as_row_vecs}
In order to understand what a linear functional looks like, we consider a simple example. Let $ T : \R^2 \to \R $ be the linear functional
\[ T( \big[\begin{smallmatrix} x \\ y\end{smallmatrix}\big]) = 2x - y. \]
Notice that we can rewrite this as a dot product between two vectors
\[ \begin{bmatrix}
	2 & -1
\end{bmatrix} \cdot 
\begin{bmatrix}
	x\\
	y
\end{bmatrix} = (2)(x) + (-1)(y) = 2x - y.
\]
In particular, we can write the action of $ T $ on $ \big[ \begin{smallmatrix} x\\ y \end{smallmatrix} \big] $ as a row vector acting on a column vector. This tells us that linear functionals have "content" associated with them. The $ 2 $ and $ -1 $ scalars are a part of how $ T $ acts on vectors. We can rewrite $ T $ in the following way
\begin{align*}
 	T( \big[\begin{smallmatrix} x \\ y\end{smallmatrix}\big]) &= \begin{bmatrix}
	2 & -1
\end{bmatrix} \cdot 
\begin{bmatrix}
	x\\
	y
\end{bmatrix}\\
	&=
	( 2 \dual{e_1} - \dual{e_2} ) (\big[\begin{smallmatrix} x \\ y \end{smallmatrix}\big])\\
	&= 2 \dual{e_1}(\big[\begin{smallmatrix} x \\ y \end{smallmatrix}\big]) - \dual{e_2}(\big[\begin{smallmatrix} x \\ y \end{smallmatrix}\big])\\
	&= 2x - y.
\end{align*}
A natural question one might ask after seeing this is whether all such linear functionals (row vectors) can be written in terms of the dual basis.
\end{example}

\begin{proposition}
The dual basis of the standard basis for $ V $ is a basis for $ \dual{V} $.
\end{proposition}

\begin{proof}
We first prove that $ e_\dual{1}, \ldots e_\dual{n} $ is linearly independent. Suppose $ c_1 e_\dual{1} + \ldots + c_n e_\dual{n} = 0 $. Then $(c_1 e_\dual{1} + \ldots + c_n e_\dual{n})(e_i) = c_i = 0  $ for all $ i $, which proves independence. The result will follow subsequently since the dimension of $ \vspan(e_\dual{1}, \ldots e_\dual{n}) $ coincides with $ \dual{V} $. However, we prove that it also spans $ \dual{V} $ to get a feel for computation in the dual space.
Let $ \varphi \in \dual{V}$. Then for any $ v \in V $, write $ v = \sum c_i e_i $. We see $ \varphi(v) = \sum c_i \varphi(e_i) $ to determine $ \varphi $ we need only consider its image on the basis in $ V $, so let $ \varphi_i = \varphi(e_i) $. Observe that $ c_i = \dual{e}_i(v) $, so we write $ \sum c_i \varphi(e_i) = \sum \varphi_i \dual{e}_i(v) $. That is, $ \varphi \in \vspan(e_\dual{1}, \ldots, e_\dual{n}) $.
\end{proof}

\begin{definition}
Let $ T : V \to W $ be a linear map. The \textit{transpose} or \textit{dual map} of $ T $ is the linear map $ \dual{T} : \dual{W} \to \dual{V} $ given by $ \dual{T}(\dual{w}) = \dual{w} \circ T $, for each $ \dual{w} \in \dual{W} $.
\end{definition}

An interpretation of the dual map following the discussion of Example \ref{forms_as_row_vecs} is that it sends row vectors in $ W $ to row vectors in $ V $.

\begin{example}
Let $ T: \R^2 \to \R^3 $ be given by $ T(x, y) = (2x, y - x, 3x + y) $. Its matrix representation is
\[T = \begin{bmatrix}
	 2 & 0\\
	-1 & 1\\
	 3 & 1 
\end{bmatrix}. \]
Considering its dual map $ \dual{T} : \dual{(\R^3)} \to \dual{(\R^2)} $, it sends $ \dual{e_i} $ to the functional $ \dual{e_i} \circ T \in \dual{ (\R^2) } $. So in particular, we have
(TODO)
\begin{align*}
\end{align*}
	
\end{example}

\begin{proposition}
We have the following properties of the transpose.
\begin{enumerate}
\item $ \dual{(S + T)} = \dual{S} + \dual{T}$, for all $ S, T \in \Hom(V, W) $.
\item $ \dual{(\lambda T)} = \lambda \dual{T} $, for all $ \lambda \in k $ and $ T \in \Hom(V, W) $.
\item $ \dual{(S T)} = \dual{T} \dual{S} $ for all $ T \in \Hom(U, V) $ and $ S \in \Hom(V, W) $.
\end{enumerate}
\end{proposition}

\begin{proof}
$ (1) $ 
\begin{align*}
	\dual{(S + T)}(\dual{w}) &= \dual{w} (S + T)\\
	&= \dual{w}(S) + \dual{w}(T)\\
	&= \dual{S}(\dual{w}) + \dual{T}(\dual{w})\\
	&= (\dual{S} + \dual{T})(\dual{w}).
\end{align*}

$ (2) $ Easy.

$ (3) $ 
\begin{align*}
	\dual{(ST)}(\dual{w}) &= \dual{w} \circ (S \circ T)\\
	&= (\dual{w} \circ S)\circ T\\
	&= (\dual{S}(\dual{w})) \circ T\\
	&= \dual{T}(\dual{S}(\dual{w}))\\
	&= (\dual{T} \dual{S})(\dual{w}).
\end{align*}
\end{proof}

\begin{definition}
Let $ U \subset V $. The \textit{annihilator} of $ U $, denoted $ U^0 $ is defined to be
\[ U^o = \{ \dual{v} \in \dual{V} \mid \dual{v}(U) = 0 \}. \]
\end{definition}

\begin{proposition}
For $ U \subset V $, $ U^o $ is a subspace of $ \dual{V} $.
\end{proposition}

\begin{proof}
Trivial.
\end{proof}

\begin{proposition} \label{ann_dim}
Let $ U $ be a subspace of $ V $. Then $ \dim V = \dim U + \dim U^o $.
\end{proposition}or 
(TODO)
\begin{proof}
\end{proof}

\begin{proposition}
Let $ T : V \to W $ be a linear map. Then
\begin{enumerate}
\item $ \ker \dual{T} = (\im T)^0 $.
\item $ \dim \ker \dual{T} = \dim \ker T + \dim W - \dim V $.
\end{enumerate}
\end{proposition}

\begin{proof}
$ (1) $ This is a straightforward computation:
\begin{align*}
	\dual{w} \in \ker \dual{T} &\iff \dual{T}(\dual{w}) = 0\\
	&\iff \dual{w} \circ T = 0\\
	&\iff \dual{w} (\im T) = 0\\
	&\iff \dual{w} \in (\im T) ^0.
\end{align*}

$ (2) $ by part $ 1 $, the Rank-Nullity Theorem, and Proposition \ref{ann_dim}, we have 
\begin{align*}
	\dim \ker \dual{T} &= \dim (\im T)^0\\
	&= \dim W - \dim \im T\\
	&= \dim W - (\dim V - \dim \ker T)\\
	&= \dim \ker T + \dim W - \dim V.
\end{align*}
\end{proof}

\begin{lemma}
Let $ V $ be a vector space. Then $ V^0 = 0 $.
\end{lemma}

\begin{proof}
For $ \dual{v} \in V^0 $, $ \dual{v}(v) = 0 $ for all $ v \in V $, hence $ \dual{v} = 0 $.
\end{proof}

\begin{proposition}
Let $ T : V \to W $ be a linear map. Then $ T $ is surjective iff $ \dual{T} $ is injective.
\end{proposition}

\begin{proof}
If $ T $ is surjective then $ \im T = W $, hence $ W^0 = (\im T)^0 = \ker \dual{T} = 0 $, proving $ \dual{T} $ is injective. Conversely, if $ \dual{T} $ is injective then $ \ker \dual{T} = (\im T)^0 = 0 $.
\end{proof}

\begin{proposition}
Let $ T : V \to W $ be a linear map.
\begin{enumerate}
\item $ \dim \im \dual{T} = \dim \im T$.
\item $ \im \dual{T} = (\ker T)^0 $.
\end{enumerate}
\end{proposition}
(TODO)

\begin{proposition}
Let $ T : V \to W $ be a linear map. Then $ T $ is injective iff $ \dual{T} $ is surjective.
\end{proposition}
(TODO)

\subsection{Relation Between the Transpose Map and the Matrix Transpose}

In order to justify the name of the transpose map $ \dual{T} : \dual{W} \to \dual{V} $, we show that it is equivalent to the matrix transpose operation. Informally we realize that the transpose map does the same thing as the matrix transpose, except "rotated" 90 degrees. First we show that the matrix transpose of a vector is equal to the corresponding vector in the dual space. Let $ v = (v_1, \ldots, v_n) \in V$. This is the same as writing $ v = v_1 e_1 + \cdots + v_n e_n $. We usually denote the matrix representation of $ v $ by 

\[ v = \begin{bmatrix}
	v_1 \\
	\vdots \\
	v_n
\end{bmatrix},\]
and write its transpose as
\[v^T = \begin{bmatrix}
	v_1 & \cdots & v_n
\end{bmatrix}.\]
We can consider $ v^T $ as a linear form $ v^T \in \dual{V} $ as usual where its action is given by 
\begin{align*}
	v^T (v') &= \begin{bmatrix}
	v_1 & \cdots & v_n
\end{bmatrix} \begin{bmatrix}
	v_1'\\
	\vdots \\
	v_n'
\end{bmatrix} = v_1 v_1' + \cdots + v_n v_n'.
\end{align*}
In particular we have $ v^T(e_i) = v_i $. Observe that this is exactly the action of the dual vector $ \dual{v} $ which sends $ e_i $ to its $ i $th coordinate. Therefore, $ v^T $ and $ \dual{v} $ act equivalently in the sense described above (and this is how row vectors act on column vectors normally).

Consider a linear map $ A : V \to W $ and its corresponding matrix transpose $ A^T : W \to V $. Its transpose acts on vectors $ v \in W $ such that
\begin{align*}
	A^T \begin{bmatrix}
		&\\
		&
	\end{bmatrix}
	= ( \begin{bmatrix}
	& &
	\end{bmatrix} A )^T.
\end{align*} 
If one ignores the incidental "rotation" of $ v^T A $, it is seen that this is exactly the action of $ \dual{A} $ on $ v^T = \dual{v} $. Explicitly,
\begin{align*}
	A^T \begin{bmatrix}
		&\\
		&
	\end{bmatrix}
	= ( \begin{bmatrix}
	& &
	\end{bmatrix} A )^T = (\dual{A} (\begin{bmatrix}
	& &
	\end{bmatrix}))^T.
\end{align*}
That is, the resulting vector from $ A^T v $ is the same vector as $ \dual{A}(\dual{v}) $ except considered as a column vector, which is to say its original vector is taken instead of its dual.
\subsection{Duality and Tensor Products}

\begin{definition}
For a map $ \varphi : U \times V \to W $ and any $ u \in U $, define the \textit{curried} map $ \varphi(u, -) : V \to W $ defined by $ \varphi(u, -)(v) = \varphi(u, v) $.
\end{definition}

\begin{proposition}[Bilinear Currying]
Let $ U, V, $ and $ W $ be any vector spaces. Then there is a natural isomorphism
\[ \Hom(U \tensor V, W) \cong \Hom(U, \Hom(V, W)) \]
\end{proposition}

\begin{proof}
For a bilinear map $ \varphi : U \times V \to W $, it is obvious that $ \varphi \in \Hom(U, W) \cap \Hom(V, W) $ since it is linear in each of its arguments. Thus we have the map 
\[\Gamma: \Hom(U \tensor V, W) \to \Hom(U, \Hom(V, W)) \] 
given by $ \varphi \mapsto (u \mapsto \varphi(u, -)) $, which can easily be seen to be well-defined. If $ f \in \ker \Gamma $ then 0 = $ \Gamma(f) = (u \mapsto (f(u, -)) $ implies $ f(u, -) = 0 $ for all $ u $. That is, $ f(u, -)(v) = 0 $ for all $ v $, hence $ f = 0 $. Lastly, given any $ f \in \Hom(U, \Hom(V, W)) $ the map $ (u, v) \mapsto f(u)(v) $ yields a bilinear map $ \Hom_2(U \times V, W) $, hence a map in $ \Hom(U \tensor V, W) $.
\end{proof}

\begin{proposition} \label{tensor_bifunctor}
The tensor product is a bifunctor $ \tensor : \Vect \times \Vect \to \Vect $. 
\end{proposition}

\begin{proof}
Given vector spaces $ (V_1, V_2) $ (TODO)
\end{proof}

\begin{proposition}
The tensor product is right exact.
\end{proposition}
(TODO)

\begin{corollary}
An immediate consequence of Proposition \ref{tensor_bifunctor} is the following. If $ U $ and $ V $ are isomorphic vector spaces, then it follows that $ U \tensor W \cong V \tensor W $ and $ W \tensor U \cong W \tensor V $ for any vector space $ W $.
\end{corollary}

\begin{proposition}
For the $ k $-vector space $ V $, we have $ k \tensor V \cong V \cong V \tensor k $.
\end{proposition}

\begin{proposition} [Vector Space Products]
Let $ (V_i)_{i \in I} $ be a family of vector spaces indexed by $ I $. For any vector space $ U $, the map
\[ \Hom(U, \prod_{i \in I} V_i) \to \prod_{i \in I} \Hom(U, V_i) \]
given by $ f \mapsto (\pi_i \circ f)_{i \in I} $, where $ \pi_i $ denotes the $ i $th canonical projection, is an isomorphism/
\end{proposition}

\begin{proposition} [Vector Space Coproducts]
Let $ (U_i)_{i \in I} $ be a family of vector spaces indexed by $ I $. For any vector space $ V $, the map
\[\Hom(\bigoplus_{i \in I}, V) \to \prod_{i \in I} \Hom(U_i, V) \]
given by $ f \mapsto (f \circ q_i)_{i \in I} $, where $q_i $ denotes the $ i $th canonical embedding, is an isomorphism.
\end{proposition}

\begin{proposition}
With notation as in the previous proposition,
\[ (  \bigoplus_{i \in I} U_i ) \tensor V \cong \bigoplus_{i \in I} (U_i \tensor V). \]
\end{proposition}

\begin{corollary}
Let $ \{ u_i \}_{i \in I}, \{ v_j \}_{j \in J} $ be bases of the vector spaces $ U $ and $ V $, respectively. Then $ \{u_i \tensor v_j\}_{(i,j) \in I \times J} $ is a basis of $ U \tensor V $; hence $ \dim(U \tensor V) = \dim (V) \dim(V) $.
\end{corollary}

\begin{proof}
(TODO)
\end{proof}

\begin{proposition}
Let $V$ and $W$ be finite-dimensional vector spaces. An elementary tensor $v \tensor w \in V \tensor W$ is zero if and only if $f(v, w) = 0$ for every bilinear map $f : V \times W \to L$, where $L$ is a vector space.
\end{proposition}

\begin{proof}
This is immediate from the universal property of tensor products.
\end{proof}

For the following propositions, observe the following abuse of notation. Given an element $f \tensor g \in \dual{V} \tensor \dual{W}$, we may regard it as a map $f \tensor g : V \tensor W \to k \tensor k \cong k$. Technically then $f \tensor g$ is just a formal element of $\dual{V} \tensor \dual{W}$, but since it is simultaneously viewed as a map, we could say that $f \tensor g \in \dual{(V \tensor W)}$. Thus we have the following

\begin{proposition} \label{tensor_duality_nec}
Let $V$ and $W$ be any, possibly infinite-dimensional vector spaces. Then $\dual{V} \tensor \dual{W} $ is embedded in $ \dual{(V \tensor W)}$. If $V$ or $W$ is finite-dimensional, then they are isomorphic.
\end{proposition}

\begin{proof}
Define the map $ \rho : \dual{V} \tensor \dual{W} \to \dual{(V \tensor W)} $ given by 
\[ \rho(f \tensor g) = ( v \tensor w \mapsto f(v) g(w) ). \]
One can readily see this is well-defined by considering the map $\rho' : \dual{V} \times \dual{W} \to \dual{(V \tensor W)}$ given by
\[ \rho(f, g) = ( v \tensor w \mapsto f(v) g(w) ). \]
By distributivity and commutativity of $k$, this map is bilinear, hence $\rho$ is well-defined. We see that $\rho$ is injective by considering $f \tensor g \in \ker \rho$. Then $\rho(f \tensor g) = z$ is the zero map. Then $ (f \tensor g)(v \tensor w) = f(v) g(w) = 0 $ for all simple tensors. It is enough to take $v_0 \in \supp(f)$ and $w_0 \in \supp(g)$ so that $f(v_0), g(w_0) \neq 0$. Then $ (f \tensor g)(v_0 \tensor w_0) = f(v_0) g(v_0) = 0$, which implies $ f(v_0) = 0$ or $ g(w_0) = 0 $. This means the support of $ f $ or $ g $ is empty, hence $ f \tensor g = 0 $.

Now suppose $ V $ is finite-dimensional.
\end{proof}

\subsection{Basis-Free Trace}

\begin{proposition} \label{linear_maps_as_tensor}
If $ V $ and $ W $ are finite-dimensional vector spaces, then 
\[ W \tensor \dual{V} \cong \Hom(V, W). \]
\end{proposition}
\begin{proof}
(TODO) Do it two ways. First construct a linear map $W \tensor \dual{V} \to \Hom(V, W)$, show it is surjective/injective, then conclude by dimensionality considerations.

Second, construct the same map, then construct a map $\Hom(V, W) \to W \tensor \dual{V}$ following the upcoming discussion and show these two maps are inverses.

Then talk about why the first isomorphism is basis-free.
\end{proof}

This proposition allows us to view linear maps as elements of the tensor product of two vector spaces, and vice-versa. Thus we may use the machinery of tensor products to prove things about the space of linear maps.

When thinking about how to represent a linear map $T : V \to W$ as an element of $W \tensor \dual{V}$, remember the following. Let $\{v_i \}_i$ be a basis of $V$, and let $\{v^i \}_i$ be the corresponding dual basis of $\dual{V}$. Let $\{ w_j \}_j$ be a basis of $W$. Then 
\begin{align*}
	T(v_j) &= \sum_i T_{ij} w_i,
\end{align*}
where $T_{ij}$ are scalars. Another way to write this is
\begin{align*}
	(T(v_j) \tensor v^j)(v_j) &= T(v_j)(v ^j(v_j))\\
	&= \delta_{jj} T(v_j)\\
	&= T(v_j).
\end{align*}
So to represent the entirety of $T$, we write 
\begin{align*}
	T &\mapsto \sum_j T(v_j) \tensor v^j\\
	&= \sum_j (\sum_i T_{ij} w_i) \tensor v^j \\
	&= \sum_{i j} T_{ij} (w_i \tensor v^j).
\end{align*}
One way to remember this is to think of the matrix representation of $T = [ T(v_1) \cdots T(v_n)]$, where $v_j$ corresponds to column $j$. Then the $j$th column of $T$ is mapped to $T(v_j) \tensor v^j$ in $W \tensor \dual{V}$. Unravelling $T(v_j)$ in terms of a basis via bilinearity allows us to conclude with the above formula. Another way to see this is through the transpose,
\begin{align*}
	T \mapsto \sum_i w_i \tensor \dual{T}(w^i).
\end{align*}

\begin{definition}
For a vector space $V$, the evaluation map defined as $\text{ev}_V : V \tensor \dual{V} \to k$ is given by $(v \tensor f) = f(v)$.
\end{definition}

\begin{definition}
Let $\phi : \End(V) \to V \tensor \dual{V}$ be the isomorphism in Proposition \ref{linear_maps_as_tensor}. The trace is defined to be $\text{tr} : \End(V) \to k$ given by $\text{tr} = \text{ev}_V \circ \phi$.
\end{definition}

As the name suggests, this definition should coincide with the usual notion of the trace of a matrix. However, since it is defined through the isomorphism from Proposition \ref{linear_maps_as_tensor}, it is a basis-free definition. To see why this is true, we use the idea discussed in the remark about how to organize the isomorphism $\End(V) \to V \tensor \dual{V}$. That is, we think of the right component of the tensor as denoting the column of the matrix, and the left component as representing the action of $T$ on that basis vector.

\begin{proposition}
Let $V$ be vector space with a basis $ \{ v_i \}_i^n $. For each basis vector, write $T(v_j) = \sum_i^n T_{ij} v_i$, so that the matrix representation of $T$ is 
\[T = \begin{bmatrix}
	T_{11} & T_{12} & \cdots & T_{1n}\\
	T_{21} & T_{22} & \cdots & T_{2n}\\
	\vdots & \vdots & \ddots & \vdots\\
	T_{n1} & T_{n2} & \cdots & T_{nn}
\end{bmatrix}. \]
Then 
\[ \text{tr}(T) = \sum_{i=1}^n T_{ii}. \]
\end{proposition}

\begin{proof}
We follow the composition $\text{ev}_V \circ \phi(T)$: 
\begin{align*}
	T \mapsto \sum_{i=1}^n T(v_i) \tensor v^i \mapsto& \text{ev}_V(\sum_{i=1}^n T(v_i) \tensor v^i)\\
	&= \sum_{i=1}^n \text{ev}_V(T(v_i) \tensor v^i)\\
	&= \sum_{i=1}^n v^i(T(v_i))\\
	&= \sum_{i=1}^n v^i(\sum_{k=1}^n T_{ki} v_k)\\
	&= \sum_{i=1}^n T_{ii}\\
	&= T_{11} + T_{22} + \cdots + T_{nn}.
\end{align*}
\end{proof}

\begin{proposition}
Let $ f, g \in \End(V) $, for a finite-dimensional vector space $V$.
\begin{enumerate}
\item $ \tr(f \circ g) = \tr(g \circ f) $.
\item $ \tr(\dual{f}) = \tr(f) $.
\end{enumerate}
\end{proposition}

\begin{proposition}
If $ V $ is a vector space, then $ \dual{(\dual{V})} \cong V$ canonically. 
\end{proposition}

\begin{proof}

\end{proof}

\section{Some Category Theory, Maybe?}



\section{Hopf Algebras}

\subsection{Basic Definitions and Properties}

TODO: Definitions of algebra, coalgebra, bialgebra, etc, examples

\begin{definition}[Sweedler Notation]
The image of a linear map $ f : C \to C \tensor C $ is usually written as
\[ f(c) = \sum_{i=1}^k c_{i1} \tensor c_{i2}, \]
since elements of a tensor product are linear combinations of simple tensors. For brevity's sake we instead write
\[ f(c) = \sum_c c^1 \tensor c^2, \]
or sometimes even just $ f(c) = c^1 \tensor c^2 $. 
\end{definition}

\begin{definition} [Coalgebra: Equation Definition]
A coalgebra is a triple $ (C, \comult, \counit)$ where $ C $ is a vector space and $ \comult : C \to C \tensor C $, $ \counit : C \to k $ are linear maps such that
\begin{enumerate}
\item \label{coassoc} $ (\id \tensor \comult) \circ \comult = (\comult \tensor \id) \circ \comult$, 
\item \label{counit} $ \sum \counit(c^1) c^2 = \sum c^1 \counit(c^2) = c$.
\end{enumerate}
\end{definition}

\begin{definition} [Coalgebra: Diagram Definition]
A coalgebra is a triple $ (C, \comult, \counit)$ where $ C $ is a vector space and $ \comult : C \to C \tensor C $, $ \counit : C \to k $ are linear maps such that the following two diagrams commute.
\[ \begin{tikzcd}
	C \tensor C \tensor C & C \tensor C \arrow[l, "\id \tensor \comult"'] & & k \tensor C & \arrow[l, "\counit \tensor \id"'] C \tensor C \arrow[r, "\id \tensor \counit"] & C \tensor k\\
	C \tensor C \arrow[u, "\comult \tensor \id"] & C \arrow[l, "\comult"] \arrow[u, "\comult"'] & & & C \arrow[u, "\comult"'] \arrow[ur, "c \mapsto c \tensor 1"']  \arrow[ul, "c \mapsto 1 \tensor c"]& 
\end{tikzcd}\]
\end{definition}

\begin{definition} [Cocommutative]
A coalgebra $ C $ is said to be cocommutative provided that $ \sum c^1 \tensor c^2 = \sum c^2 \tensor c^1 $, or in other words, $ \comult = \twist \circ \comult $.
\end{definition}

\begin{definition} [Subcoalgebra]
If $ C $ is a coalgebra and $ D \subset C $, then $ D $ is called a subcoalgebra of $ C $ if $ \comult(D) \subset D \tensor D $.
\end{definition}

\begin{definition} [Coalgebra Morphism]
A coalgebra morphism $ f : C \to D $ is a linear map such that the following conditions hold:
\begin{enumerate}
\item \label{coalg_morph_1} $ \comult_D(f(c)) = \sum f(c)^1 \tensor f(c)^2 = \sum f(c^1) \tensor f(c^2) = (f \tensor f)(\comult_C(c)) $, for all $ c \in C$.
\item \label{coalg_morph_2}$ \counit_D(f(c)) = \counit_C(c) $, for all $ c \in C $.
\end{enumerate}
\end{definition}

\begin{proposition}
If $ C  $ is a subcoalgebra of $ D $ then the inclusion $ \iota : C \to D $ is a coalgebra morphism.
\end{proposition}

\begin{proof}
Let $ c \in C $ and observe that $  $
\begin{align*}
	\sum \iota(c^1) \tensor \iota(c^2) &= \sum c^1 \tensor c^2 \\
	&= \comult(c) = \comult(\iota(c))\\
	&= \sum \iota(c)^1 \tensor \iota(c)^2.
\end{align*}
Thus condition \ref{coalg_morph_1} of the definition is satisifed. It is also clear that $ \counit (c) = \counit(\iota(c))$, hence $ \iota $ is a coalgebra morphism.
\end{proof}

\begin{proposition}
If $(C, \comult, \counit)$ is a coalgebra then $ \dual{C} $ is an algebra.
\end{proposition}

\begin{proof}
We consider the two maps required to make $ \dual{C} $ into an algebra. First we need a multiplication map, which we can find by looking at the domain and codomain of the given comultiplication. Indeed $ \comult: C \to C \tensor C $ while we need a map $\mu: \dual{C} \tensor \dual{C} \to \dual{C}$. Thus we consider the transpose $\dual{\comult} : \dual{(C \tensor C)} \to \dual{C}$. By Proposition \ref{tensor_duality_nec}, we know $\dual{C} \tensor \dual{C} \subseteq \dual{(C \tensor C)}$ so if we consider the restriction $m = \dual{\comult} \restrict{\dual{C} \tensor \dual{C}}$, we get $m : \dual{C} \tensor \dual{C} \to \dual{C}$ as desired. It follows immediately that $m$ is linear since the restricted transpose is linear.

Next we find a linear map $\eta : k \to \dual{C}$ to act as our unit map. Again, we look at the counit map $\counit : C \to k$ and take its transpose $\dual{\counit} : \dual{k} \to \dual{C}$. It is clear that $\phi : k \to \dual{k}$ given by $\phi(1) = \id_k$ is a linear isomorphism, so we define $ \eta = \dual{\varepsilon} \circ \phi $.

Now we check that the structure maps satisfy the axioms of an algebra. Write $ \mu(a \tensor b) = ab $ in $ \dual{C} $.
\begin{align*}
	\mu (\eta \tensor \id)()  &= \dual{\counit}(id) 
\end{align*}
\end{proof}

\section{Miscellaneous}

\begin{remark}
Assume all groups and vector spaces are finite-dimensional unless otherwise stated.
\end{remark}

\begin{proposition}
If $ V $ be an irreducible representation then $ \dual{V} $ is an irreducible, but is not the same representation.
\end{proposition}

\begin{proposition}
Let $ A $ be a finite abelian group, and let $ V $ be an irreducible representation of $ A $ over an algebraically closed field $k$. Then $ \dim V = 1 $.
\end{proposition}

\begin{proof}
Let $ \rho : k[A] \to V $ be the corresponding representation. Then for any $a \in A$, $a^n = 1$ where $n = |A|$. Thus, $\rho(a)^n = \id_V \implies \rho(a)^n - \id_V = $
\end{proof}

\begin{proof}

\end{proof}

\section{Nilpotent Endomorphisms}

\begin{definition}
A map $ T $ is called nilpotent if $ T^m = 0 $ for some integer $ m $.
\end{definition}

Note that there are only two types of endomorphisms on a vector space. Those that are nilpotent, and those that are invertible. If $ T \in \End(V)$ is injective or surjective, then it is immediately invertible by. We assume that all vector spaces in this section are finite-dimensional.

\begin{lemma}
$ T \in \End(V)$ is nilpotent iff for every $ v \in V $, there exists an $ m $ such that $ T^m(v) = 0 $.
\end{lemma}

\begin{proof}
	If $ T $ is nilpotent, then clearly $ T^m(v) = 0 $ for some integer $ m $. Conversely, if there exists an $ m $ such that $ T^m(v) =0 $ for every $ v \in V $, then we consider a basis of $ V $. Let $ e_1, \ldots, e_n $ be a basis of $ V $ and let $ m_i $ be the corresponding integer for which $ T^{m_i}(e_i) = 0 $. Setting $ m = \max (m_1, \ldots, m_n) $, we have $ T^m(v) = \sum a_i T^m(e_i) = 0 $.
\end{proof}

\begin{definition}
A subspace $ U \subset V $ is $ T $-invariant if 
\[ u \in U \implies T(u) \in U. \]
\end{definition}

\begin{lemma}
If $ T : V \to V $ is nilpotent, then $ T \mid_U $ is nilpotent, where $ U $ is a $ T $-invariant subspace of $ V $.
\end{lemma}

\begin{theorem}
$ T \in \End(V) $ is nilpotent iff $ V $ has a basis in which the matrix of $ T $ is strictly upper-triangular. That is, there exists a basis $ e_1, \ldots, e_n $ such that
\[ T(e_i) = \sum _{j < i} a_{ij}e_j. \]
\end{theorem}

\section{Eigenstuff Et. El}

\begin{definition}
Let $ V $ be a vector space over a field $ F $, and let $ T \in End(V) $.
\begin{enumerate}
\item An \textit{eigenvalue} of $ T $ is a scalar $ \lambda \in F $ such that $ T(x) = \lambda x $, for some $ x \in V$. $ x $ is called an \textit{eigenvector}.
\item Given an eigenvalue $ \lambda $ of $ T $, the set $ E_\lambda(T) = \{ v \in V \mid T(v) = \lambda v$ is called the \textit{$ \lambda $-eigenspace} associated with $ T $.
\end{enumerate}
\end{definition}



\end{document}



